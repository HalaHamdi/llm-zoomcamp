{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d658f909-e679-41e9-9c4e-e0241c719049",
   "metadata": {},
   "source": [
    "If you're not running in Saturn Cloud, you need to install these libraries:\n",
    "\n",
    "Make sure you use the latest versions\n",
    "\n",
    "```\n",
    "pip install -U transformers accelerate bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6329634-af7f-4b94-8c30-4b9380dbf9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many space i have in the home directory\n",
    "# !df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a810f0-afca-4f1c-bbe7-beef7afb273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0446438c-af04-43f1-84eb-9a12198d19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dir to have more space while downlaading\n",
    "# import os \n",
    "# os.environ['HF_HOME'] ='/dev/sda1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f2103a-34af-4658-8b8f-d9bbac03b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bd246ae7fe735e0001afee8b552f79fed6cbe6fb0c7e73f5b392ecf9d659b5cf\n"
     ]
    }
   ],
   "source": [
    "# !docker run -it \\\n",
    "#     --rm \\\n",
    "#     -v ollama:/root/.ollama \\\n",
    "#     -p 11434:11434 \\\n",
    "#     --name ollama \\\n",
    "#     ollama/ollama\n",
    "# !docker stop ollama  \n",
    "! docker run -d --rm -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8f679d-d8da-4d07-b7b4-49dbd6e330c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.2.1\n"
     ]
    }
   ],
   "source": [
    "# Q1: What's the version of ollama client?\n",
    "# To find out, enter the container and execute ollama with the -v flag.\n",
    "!docker exec ollama ollama -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b23e1e5-eff0-4553-a1ca-80e6231813fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling c1864a5eb193... 100% ▕████████████████▏ 1.7 GB                         \n",
      "pulling 097a36493f71... 100% ▕████████████████▏ 8.4 KB                         \n",
      "pulling 109037bec39c... 100% ▕████████████████▏  136 B                         \n",
      "pulling 22a838ceb7fb... 100% ▕████████████████▏   84 B                         \n",
      "pulling 887433b89a90... 100% ▕████████████████▏  483 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "# enter the container and pull the llm model\n",
    "!docker exec ollama ollama pull gemma:2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec75fc1b-fdab-44dc-a9f3-ecba6308141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma\n"
     ]
    }
   ],
   "source": [
    "# see what is in the directory\n",
    "!docker exec ollama ls /root/.ollama/models/manifests/registry.ollama.ai/library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9232c6-7b8a-4102-9817-ac533275939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
     ]
    }
   ],
   "source": [
    "# read the content of the file\n",
    "# Q2: What's the content of the file related to gemma?\n",
    "!docker exec ollama cat /root/.ollama/models/manifests/registry.ollama.ai/library/gemma/2b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2456f12-ea8d-40e7-9404-218dd4e52b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker exec -it ollama ollama run gemma:2b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da5cf606-cb87-456d-8786-538878550014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here is the answer to the question:\\n\\n10 * 10 = 100.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the following prompt: \"10 * 10\". What's the answer?\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gemma:2b',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "llm (\"10 * 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21aa255e-c971-44ca-9826-a721df3ad063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama\n",
      "Error response from daemon: No such container: ollama\n",
      "mkdir: cannot create directory ‘ollama_files’: File exists\n",
      "691c444e4fcf6dfee39dc05f764743acdd37dd92047d8611b2e89dfe0d7cbc62\n"
     ]
    }
   ],
   "source": [
    "# Stop and remove existing container named ollama\n",
    "!docker stop ollama\n",
    "!docker rm ollama\n",
    "\n",
    "# Create the local directory for mounting\n",
    "!mkdir ollama_files && ls ollama_files\n",
    "\n",
    "# Run a new Docker container with volume mapping and port forwarding\n",
    "!docker run -it \\\n",
    "    -d \\\n",
    "    --rm \\\n",
    "    -v $(pwd)/ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb1fb182-1b52-4145-a907-358ecc5fdcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling c1864a5eb193... 100% ▕████████████████▏ 1.7 GB                         \n",
      "pulling 097a36493f71... 100% ▕████████████████▏ 8.4 KB                         \n",
      "pulling 109037bec39c... 100% ▕████████████████▏  136 B                         \n",
      "pulling 22a838ceb7fb... 100% ▕████████████████▏   84 B                         \n",
      "pulling 887433b89a90... 100% ▕████████████████▏  483 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "# pull the model\n",
    "!docker exec -it ollama ollama pull gemma:2b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9c53563-46e8-4779-97c1-f46ec8400598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "du: cannot access 'ollama_files/models': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# see the summary(s) of the disk usage (du)  as human readable (h)\n",
    "!docker exec ollama du -sh ollama_files/models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a58e7f3-3d8d-46a0-ba83-1e1d175f056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec ollama find / -type d -name \"ollama_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f24cf-83cc-49b7-a336-e27c353ce458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
